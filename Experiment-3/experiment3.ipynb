{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "91278e35",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zip file not found at ./data/dogs-vs-cats.zip\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Extract the dogs-vs-cats.zip file from the data folder\n",
        "zip_path = './data/dogs-vs-cats.zip'\n",
        "extract_path = './data/'\n",
        "\n",
        "if os.path.exists(zip_path):\n",
        "    try:\n",
        "        # Check if file is not empty\n",
        "        if os.path.getsize(zip_path) == 0:\n",
        "            print(f\"Error: Zip file is empty (0 bytes). Please download the dogs-vs-cats dataset.\")\n",
        "        else:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_path)\n",
        "            print(f\"Successfully extracted {zip_path}\")\n",
        "            print(\"Contents:\", os.listdir(extract_path))\n",
        "    except zipfile.BadZipFile:\n",
        "        print(f\"Error: {zip_path} is corrupted or not a valid zip file. Please download it again.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "else:\n",
        "    print(f\"Zip file not found at {zip_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f571379a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using local environment - set data paths as needed\n"
          ]
        }
      ],
      "source": [
        "# For local environment: specify your data directory path\n",
        "# uploaded = './data/'  # or your desired data path\n",
        "print(\"Using local environment - set data paths as needed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e9c7e6e6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "from torch.utils.data import DataLoader\n",
        "import itertools\n",
        "import copy\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "472b9a8d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aryan/DEEP-LEARNING LAB/.venv/lib/python3.13/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
            "  entry = pickle.load(f, encoding=\"latin1\")\n"
          ]
        }
      ],
      "source": [
        "transform_cifar = transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "train_cifar = datasets.CIFAR10(root='./data', train=True, download=False, transform=transform_cifar)\n",
        "test_cifar  = datasets.CIFAR10(root='./data', train=False, download=False, transform=transform_cifar)\n",
        "\n",
        "train_loader_cifar = DataLoader(train_cifar, batch_size=64, shuffle=True)\n",
        "test_loader_cifar  = DataLoader(test_cifar, batch_size=64, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "35de9a9d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted train.zip\n",
            "Extracted and renamed test1.zip to test_organized\n",
            "Organized train data into cats and dogs folders\n",
            "Organized test data\n",
            "Classes: ['cats', 'dogs']\n",
            "Training samples: 25000, Test samples: 12500\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "transform_dogs = transforms.Compose([\n",
        "    transforms.Resize((128,128)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "# Extract and organize dogs-vs-cats dataset\n",
        "# NOTE: Cell 0 already extracts dogs-vs-cats.zip into ./data,\n",
        "#       so train.zip and test1.zip live directly under ./data.\n",
        "zip_dir = './data/dogs-vs-cats'\n",
        "train_zip = os.path.join(zip_dir, 'train.zip')\n",
        "test_zip = os.path.join(zip_dir, 'test1.zip')\n",
        "train_path = os.path.join(zip_dir, 'train')\n",
        "test_path = os.path.join(zip_dir, 'test')\n",
        "\n",
        "# Extract train.zip if not already extracted\n",
        "if os.path.exists(train_zip) and not os.path.exists(os.path.join(zip_dir, 'train', 'cats')):\n",
        "    with zipfile.ZipFile(train_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall(zip_dir)\n",
        "    print(\"Extracted train.zip\")\n",
        "\n",
        "# Extract test1.zip if not already extracted\n",
        "if os.path.exists(test_zip) and not os.path.exists(os.path.join(zip_dir, 'test_organized')):\n",
        "    with zipfile.ZipFile(test_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall(zip_dir)\n",
        "    os.rename(os.path.join(zip_dir, 'test1'), os.path.join(zip_dir, 'test_organized'))\n",
        "    print(\"Extracted and renamed test1.zip to test_organized\")\n",
        "\n",
        "# Organize train directory into class folders (cats/dogs) if needed\n",
        "train_cats_dir = os.path.join(train_path, 'cats')\n",
        "train_dogs_dir = os.path.join(train_path, 'dogs')\n",
        "\n",
        "# Ensure class folders exist\n",
        "if not os.path.exists(train_cats_dir):\n",
        "    os.makedirs(train_cats_dir, exist_ok=True)\n",
        "if not os.path.exists(train_dogs_dir):\n",
        "    os.makedirs(train_dogs_dir, exist_ok=True)\n",
        "\n",
        "# Move cat and dog images to respective folders (idempotent)\n",
        "for file in os.listdir(train_path):\n",
        "    if file.startswith('cat.') and file.endswith('.jpg'):\n",
        "        shutil.move(os.path.join(train_path, file), os.path.join(train_cats_dir, file))\n",
        "    elif file.startswith('dog.') and file.endswith('.jpg'):\n",
        "        shutil.move(os.path.join(train_path, file), os.path.join(train_dogs_dir, file))\n",
        "print(\"Organized train data into cats and dogs folders\")\n",
        "\n",
        "# Load training dataset using ImageFolder\n",
        "train_dogs = datasets.ImageFolder(train_path, transform=transform_dogs)\n",
        "\n",
        "# For test data (which has no labels), create dummy labels directory structure\n",
        "test_dummy_path = os.path.join(zip_dir, 'test_dummy')\n",
        "test_dummy_images = os.path.join(test_dummy_path, 'images')\n",
        "if not os.path.exists(test_dummy_images):\n",
        "    os.makedirs(test_dummy_images, exist_ok=True)\n",
        "    # Copy all test images to a single class directory\n",
        "    test_org_path = os.path.join(zip_dir, 'test_organized')\n",
        "    for file in os.listdir(test_org_path):\n",
        "        if file.endswith('.jpg'):\n",
        "            src = os.path.join(test_org_path, file)\n",
        "            dst = os.path.join(test_dummy_images, file)\n",
        "            if not os.path.exists(dst):\n",
        "                shutil.copy(src, dst)\n",
        "    print(\"Organized test data\")\n",
        "\n",
        "test_dogs = datasets.ImageFolder(test_dummy_path, transform=transform_dogs)\n",
        "\n",
        "train_loader_dogs = DataLoader(train_dogs, batch_size=32, shuffle=True)\n",
        "test_loader_dogs  = DataLoader(test_dogs, batch_size=32, shuffle=False)\n",
        "\n",
        "print(\"Classes:\", train_dogs.classes)\n",
        "print(f\"Training samples: {len(train_dogs)}, Test samples: {len(test_dogs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "661ac425",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, num_classes, activation):\n",
        "        super(CustomCNN, self).__init__()\n",
        "\n",
        "        if activation == \"relu\":\n",
        "            act = nn.ReLU()\n",
        "        elif activation == \"tanh\":\n",
        "            act = nn.Tanh()\n",
        "        else:\n",
        "            act = nn.LeakyReLU()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            act,\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            act,\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            act,\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128*8*8, 256),\n",
        "            act,\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6ac00763",
      "metadata": {},
      "outputs": [],
      "source": [
        "def init_weights(model, init_type):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "            if init_type == \"xavier\":\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "            elif init_type == \"kaiming\":\n",
        "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
        "            else:\n",
        "                nn.init.normal_(m.weight, mean=0.0, std=0.02)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7db1ac1b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "\n",
        "    return running_loss/len(loader), correct/len(loader.dataset)\n",
        "\n",
        "\n",
        "def eval_model(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "\n",
        "    return running_loss/len(loader), correct/len(loader.dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "256f94ed",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Config: relu | xavier | sgd\n",
            "Epoch 1: Val Acc=0.5353\n",
            "Epoch 2: Val Acc=0.4928\n",
            "Epoch 3: Val Acc=0.5841\n",
            "Epoch 4: Val Acc=0.6425\n",
            "Epoch 5: Val Acc=0.6721\n",
            "\n",
            "Config: relu | xavier | adam\n",
            "Epoch 1: Val Acc=0.4596\n",
            "Epoch 2: Val Acc=0.5330\n",
            "Epoch 3: Val Acc=0.5719\n",
            "Epoch 4: Val Acc=0.5802\n",
            "Epoch 5: Val Acc=0.6258\n",
            "\n",
            "Config: relu | xavier | rmsprop\n",
            "Epoch 1: Val Acc=0.4144\n",
            "Epoch 2: Val Acc=0.4792\n",
            "Epoch 3: Val Acc=0.5192\n",
            "Epoch 4: Val Acc=0.6096\n",
            "Epoch 5: Val Acc=0.6449\n",
            "\n",
            "Config: relu | kaiming | sgd\n",
            "Epoch 1: Val Acc=0.5315\n",
            "Epoch 2: Val Acc=0.5778\n",
            "Epoch 3: Val Acc=0.6040\n",
            "Epoch 4: Val Acc=0.6177\n",
            "Epoch 5: Val Acc=0.6269\n",
            "\n",
            "Config: relu | kaiming | adam\n"
          ]
        }
      ],
      "source": [
        "\n",
        "activations = [\"relu\", \"tanh\", \"leaky\"]\n",
        "inits = [\"xavier\", \"kaiming\", \"random\"]\n",
        "optimizers_list = [\"sgd\", \"adam\", \"rmsprop\"]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "best_models = {}\n",
        "\n",
        "for act, init, opt in itertools.product(activations, inits, optimizers_list):\n",
        "    print(f\"\\nConfig: {act} | {init} | {opt}\")\n",
        "\n",
        "    model = CustomCNN(num_classes=10, activation=act).to(device)\n",
        "    init_weights(model, init)\n",
        "\n",
        "    if opt == \"sgd\":\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "    elif opt == \"adam\":\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    else:\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(5):\n",
        "        train_loss, train_acc = train_model(model, train_loader_cifar, criterion, optimizer)\n",
        "        val_loss, val_acc = eval_model(model, test_loader_cifar, criterion)\n",
        "        print(f\"Epoch {epoch+1}: Val Acc={val_acc:.4f}\")\n",
        "\n",
        "    best_models[f\"{act}_{init}_{opt}\"] = copy.deepcopy(model.state_dict())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
